{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import regionprops\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "\n",
    "def load_data_from_dir(input_dir):\n",
    "    \"\"\"Load 3D instance segmentation from a directory of 2D TIFF slices.\"\"\"\n",
    "    file_list = sorted([f for f in os.listdir(input_dir) if f.endswith('.tiff')])\n",
    "    if not file_list:\n",
    "        raise ValueError(f\"No TIFF images found in {input_dir}\")\n",
    "    return np.stack([tiff.imread(os.path.join(input_dir, f)) for f in file_list])\n",
    "\n",
    "def compute_psd(segmentation):\n",
    "    \"\"\"Compute volume, surface area, diameter, feret diameter, solidity, and sphericity for each particle in the 3D segmentation.\"\"\"\n",
    "    labels = segmentation\n",
    "    props = regionprops(labels)\n",
    "\n",
    "    def compute_metrics(prop):\n",
    "        if prop.label == 0:  # Exclude background\n",
    "            return None\n",
    "\n",
    "        instance_label = prop.label  # Instance ID\n",
    "        volume = prop.area  # Voxel count as volume\n",
    "        bbox = prop.bbox\n",
    "        min_x, min_y, min_z, max_x, max_y, max_z = bbox\n",
    "        surface_area = (\n",
    "            np.sum(segmentation[min_x+1:max_x, min_y:max_y, min_z:max_z] != \n",
    "                   segmentation[min_x:max_x-1, min_y:max_y, min_z:max_z]) +\n",
    "            np.sum(segmentation[min_x:max_x, min_y+1:max_y, min_z:max_z] != \n",
    "                   segmentation[min_x:max_x, min_y:max_y-1, min_z:max_z]) +\n",
    "            np.sum(segmentation[min_x:max_x, min_y:max_y, min_z+1:max_z] != \n",
    "                   segmentation[min_x:max_x, min_y:max_y, min_z:max_z-1])\n",
    "        )\n",
    "        # Get other regionprops properties\n",
    "        diameter = prop.equivalent_diameter_area # (6 * volume / np.pi) ** (1/3)\n",
    "        feret = prop.feret_diameter_max\n",
    "        solidity = prop.solidity\n",
    "        if solidity > 1:\n",
    "            solidity = 0\n",
    "\n",
    "        # Compute sphericity: the ratio of the surface area of a sphere with the same volume as the object to the surface area of the object itself\n",
    "        sphericity = (np.pi ** (1/3) * (6 * volume) ** (2/3)) / surface_area if surface_area > 0 else 0\n",
    "        if sphericity > 1:\n",
    "            sphericity = 0\n",
    "\n",
    "        return instance_label, volume, surface_area, diameter, feret, solidity, sphericity\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(delayed(compute_metrics)(prop) for prop in props)\n",
    "    results = [r for r in results if r is not None]  # Remove None values\n",
    "\n",
    "    if results:\n",
    "        instance_labels, volumes, surface_areas, diameters, ferets, solidities, sphericities = zip(*results)\n",
    "    else:\n",
    "        instance_labels, volumes, surface_areas, diameters, ferets, solidities, sphericities = ([], [], [], [], [], [])\n",
    "\n",
    "    return [np.array(instance_labels), np.array(volumes), np.array(surface_areas), np.array(diameters), np.array(ferets), np.array(solidities), np.array(sphericities)]\n",
    "\n",
    "def filter_by_iqr(raw_psd_metrics, threshold_factor=7.5):\n",
    "    \"\"\"Remove outliers using the IQR method and return filtered data.\"\"\"\n",
    "    if len(raw_psd_metrics[0]) == 0:\n",
    "        return raw_psd_metrics  # Return empty if no data\n",
    "\n",
    "    Q1 = np.percentile(raw_psd_metrics[1], 25)\n",
    "    Q3 = np.percentile(raw_psd_metrics[1], 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold_factor * IQR\n",
    "    upper_bound = Q3 + threshold_factor * IQR\n",
    "\n",
    "    mask = (raw_psd_metrics[1] >= lower_bound) & (raw_psd_metrics[1] <= upper_bound)\n",
    "    return [raw_psd_metrics[0][mask], raw_psd_metrics[1][mask], raw_psd_metrics[2][mask], raw_psd_metrics[3][mask], raw_psd_metrics[4][mask], raw_psd_metrics[5][mask], raw_psd_metrics[6][mask]]\n",
    "\n",
    "def filter_by_threshold(raw_psd_metrics, threshold=('diameter', 50)):\n",
    "    \"\"\"Filter particles based on a specified threshold for various metrics.\"\"\"\n",
    "    # Apply the appropriate threshold based on the specified metric\n",
    "    if threshold[0] == 'volume':\n",
    "        mask = raw_psd_metrics[1] <= threshold[1]\n",
    "    elif threshold[0] == 'surface area':\n",
    "        mask = raw_psd_metrics[2] <= threshold[1]\n",
    "    elif threshold[0] == 'diameter':\n",
    "        mask = raw_psd_metrics[3] <= threshold[1]\n",
    "    else:\n",
    "        raise ValueError('The threshold type must be one of the following: `volume`, `surface area`, or `diameter`!')\n",
    "    \n",
    "    # Return the filtered data based on the threshold\n",
    "    return [raw_psd_metrics[0][mask], raw_psd_metrics[1][mask], raw_psd_metrics[2][mask], raw_psd_metrics[3][mask], raw_psd_metrics[4][mask], raw_psd_metrics[5][mask], raw_psd_metrics[6][mask]], threshold\n",
    "\n",
    "def bin_data(data, bin_edges):\n",
    "    \"\"\"Bin the data according to the bin edges and return the bin counts.\"\"\"\n",
    "    counts, _ = np.histogram(data, bins=bin_edges)\n",
    "    return counts\n",
    "\n",
    "def save_psd_to_csv(psd_metrics, save_path):\n",
    "    \"\"\"Apply outlier removal using IQR and save results to CSV.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"Instance\": psd_metrics[0],\n",
    "        \"Volume\": psd_metrics[1],\n",
    "        \"Surface Area\": psd_metrics[2],\n",
    "        \"Diameter\": psd_metrics[3],\n",
    "        \"Feret Diameter\": psd_metrics[4],\n",
    "        \"Solidity\": psd_metrics[5],\n",
    "        \"Sphericity\": psd_metrics[6]\n",
    "    })\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved PSD as CSV to {save_path}\")\n",
    "\n",
    "def save_histogram(data, bin_edges, title, xlabel, save_path):\n",
    "    \"\"\"Save histogram plot with scaled x-axis, excluding zero values.\"\"\"\n",
    "    data = data[data > 0]  # Exclude zero values\n",
    "    if len(data) == 0:\n",
    "        print(f\"Warning: No valid data for histogram {title}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(data, bins=bin_edges, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Particle Count\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_binned_psd(psd_metrics, save_path, num_bins=50, threshold=None):\n",
    "    \"\"\"Save binned Particle Size Distribution (PSD) to CSV for multiple metrics.\"\"\"\n",
    "    volume_min = 0\n",
    "    surface_area_min = 0\n",
    "    diameter_min = 0\n",
    "    feret_min = 0\n",
    "    solidity_min = 0\n",
    "    sphericity_min = 0\n",
    "    \n",
    "    # Range from 0 to 1\n",
    "    solidity_max = 1\n",
    "    sphericity_max = 1\n",
    "    \n",
    "    if threshold is None:\n",
    "        # Calculate the range for each dataset and round accordingly\n",
    "        volume_max = np.ceil(max(psd_metrics[1]))   # Round up\n",
    "        surface_area_max = np.ceil(max(psd_metrics[2]))   # Round up\n",
    "        diameter_max = np.ceil(max(psd_metrics[3]))   # Round up\n",
    "        feret_max = np.ceil(max(psd_metrics[4]))   # Round up\n",
    "    else:\n",
    "        if threshold[0] == 'volume':\n",
    "            volume_max = threshold[1]\n",
    "            diameter_max = np.ceil((6 * volume_max / np.pi) ** (1/3))\n",
    "            surface_area_max = np.ceil(4 * np.pi * (diameter_max / 2) ** 2)\n",
    "            feret_max = max(psd_metrics[4]) * diameter_max / (max(psd_metrics[3])) # Approximate max feret as proprotional to thresholded max for diameter\n",
    "        elif threshold[0] == 'surface area':\n",
    "            surface_area_max = threshold[1]\n",
    "            diameter_max = np.ceil(np.sqrt(surface_area_max / (4 * np.pi)) * 2)\n",
    "            volume_max = np.ceil((4/3) * np.pi * (diameter_max / 2) ** 3)\n",
    "            feret_max = max(psd_metrics[4]) * diameter_max / (max(psd_metrics[3])) # Approximate max feret as proprotional to thresholded max for diameter\n",
    "            print('Please note that thresholding based on surface area may lead to odd binned histograms due to the very approximate estimation of surface area that is not directly related to volume or diameter.')\n",
    "        elif threshold[0] == 'diameter':\n",
    "            diameter_max = threshold[1]\n",
    "            volume_max = np.ceil((4/3) * np.pi * (diameter_max / 2) ** 3)\n",
    "            surface_area_max = np.ceil(4 * np.pi * (diameter_max / 2) ** 2)\n",
    "            feret_max = max(psd_metrics[4]) * diameter_max / (max(psd_metrics[3])) # Approximate max feret as proprotional to thresholded max for diameter\n",
    "        else:\n",
    "            raise ValueError('The threshold type must be one of the following: `volume`, `surface area`, or `diameter`!')\n",
    "\n",
    "    def create_bins(min_val, max_val, num_bins):\n",
    "        bin_range = max_val - min_val\n",
    "        if bin_range >= num_bins:\n",
    "            return np.linspace(min_val, max_val, num_bins + 1, dtype=int)\n",
    "        else:\n",
    "            extra_bins = num_bins - bin_range  # Add extra bins if range is small\n",
    "            return np.linspace(min_val, max_val + extra_bins, num_bins + 1, dtype=int)\n",
    "\n",
    "    def create_log_bins(min_val, max_val, num_bins, epsilon=1e-6):\n",
    "        # Shift values to avoid log(0) error\n",
    "        min_val = max(min_val, epsilon)\n",
    "        log_min = np.log10(min_val)\n",
    "        log_max = np.log10(max_val)\n",
    "        log_bins = np.logspace(log_min, log_max, num_bins + 1)\n",
    "        return np.unique(log_bins)\n",
    "    \n",
    "    def create_power_bins(min_val, max_val, num_bins, power=2):\n",
    "        # Create bins using a power law scaling\n",
    "        bins = np.linspace(min_val ** power, max_val ** power, num_bins + 1) ** (1 / power)\n",
    "        return np.unique(np.round(bins).astype(int))\n",
    "\n",
    "    # Create bins for volume and surface area using logarithmic spacing\n",
    "    volume_bins = create_bins(volume_min, volume_max, num_bins)\n",
    "    surface_area_bins = create_bins(surface_area_min, surface_area_max, num_bins)\n",
    "    diameter_bins = np.linspace(diameter_min, diameter_max, num_bins + 1)\n",
    "    feret_bins = np.linspace(feret_min, feret_max, num_bins + 1)\n",
    "    solidity_bins = np.linspace(solidity_min, solidity_max, num_bins + 1)\n",
    "    sphericity_bins = np.linspace(sphericity_min, sphericity_max, num_bins + 1)\n",
    "\n",
    "    volume_counts = bin_data(psd_metrics[1], volume_bins)\n",
    "    surface_area_counts = bin_data(psd_metrics[2], surface_area_bins)\n",
    "    diameter_counts = bin_data(psd_metrics[3], diameter_bins)\n",
    "    feret_counts = bin_data(psd_metrics[4], feret_bins)\n",
    "    solidity_counts = bin_data(psd_metrics[5], solidity_bins)\n",
    "    sphericity_counts = bin_data(psd_metrics[6], sphericity_bins)\n",
    "\n",
    "    volume_bin_ranges = [f\"{v1}.0-{v2}.0\" for v1, v2 in zip(volume_bins[:-1], volume_bins[1:])]\n",
    "    surface_area_bin_ranges = [f\"{s1}.0-{s2}.0\" for s1, s2 in zip(surface_area_bins[:-1], surface_area_bins[1:])]\n",
    "    diameter_bin_ranges = [f\"{d1}-{d2}\" for d1, d2 in zip(diameter_bins[:-1], diameter_bins[1:])]\n",
    "    feret_bin_ranges = [f\"{d1}-{d2}\" for d1, d2 in zip(feret_bins[:-1], feret_bins[1:])]\n",
    "    solidity_bin_ranges = [f\"{d1}-{d2}\" for d1, d2 in zip(solidity_bins[:-1], solidity_bins[1:])]\n",
    "    sphericity_bin_ranges = [f\"{d1}-{d2}\" for d1, d2 in zip(sphericity_bins[:-1], sphericity_bins[1:])]\n",
    "    \n",
    "    binned_df = pd.DataFrame({\n",
    "        \"Volume Bin Range\": volume_bin_ranges,\n",
    "        \"Volume Count\": volume_counts,\n",
    "        \"Surface Area Bin Range\": surface_area_bin_ranges,\n",
    "        \"Surface Area Count\": surface_area_counts,\n",
    "        \"Diameter Bin Range\": diameter_bin_ranges,\n",
    "        \"Diameter Count\": diameter_counts,\n",
    "        \"Feret Diameter Bin Range\": feret_bin_ranges,\n",
    "        \"Feret Diameter Count\": feret_counts,\n",
    "        \"Solidity Bin Range\": solidity_bin_ranges,\n",
    "        \"Soldiity Count\": solidity_counts,\n",
    "        \"Sphericity Bin Range\": sphericity_bin_ranges,\n",
    "        \"Sphericity Count\": sphericity_counts\n",
    "    })\n",
    "\n",
    "    binned_df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved binned PSD results to {save_path}\")\n",
    "\n",
    "    return [volume_bins, surface_area_bins, diameter_bins, feret_bins, solidity_bins, sphericity_bins]\n",
    "\n",
    "def save_summary_metrics(psd_metrics, psd_metric_names, save_dir, run_tag, name, prefix):\n",
    "    \"\"\"Compute and save summary metrics (mean, std, median, IQR, percentiles, skewness, kurtosis) for PSD data.\"\"\"\n",
    "    \n",
    "   # Compute basic statistics\n",
    "    total_particles = len(psd_metrics[0])\n",
    "    total_volume = np.sum(psd_metrics[1])\n",
    "    total_surface_area = np.sum(psd_metrics[2])\n",
    "    \n",
    "    summary = {\n",
    "        \"Total Particles\": total_particles,\n",
    "        \"Total Volume\": total_volume,\n",
    "        \"Total Surface Area\": total_surface_area\n",
    "    }\n",
    "    \n",
    "    for psd_metric, psd_metric_name in zip(psd_metrics[1:], psd_metric_names):\n",
    "        skewness = skew(psd_metric)\n",
    "        kurt = kurtosis(psd_metric)\n",
    "        percentiles = np.percentile(psd_metric, [25, 50, 75])\n",
    "\n",
    "        # Add calculated metrics to summary\n",
    "        summary.update({\n",
    "            f\"Mean {psd_metric_name}\": np.mean(psd_metric),\n",
    "            f\"Std {psd_metric_name}\": np.std(psd_metric),\n",
    "            f\"Median {psd_metric_name}\": np.median(psd_metric),\n",
    "            f\"IQR {psd_metric_name}\": percentiles[2] - percentiles[0],\n",
    "            \n",
    "            f\"{psd_metric_name} 25th Percentile\": percentiles[0],\n",
    "            f\"{psd_metric_name} 50th Percentile (Median)\": percentiles[1],\n",
    "            f\"{psd_metric_name} 75th Percentile\": percentiles[2],\n",
    "            \n",
    "            f\"{psd_metric_name} Skewness\": skewness,\n",
    "            f\"{psd_metric_name} Kurtosis\": kurt,\n",
    "        })\n",
    "    \n",
    "    # Convert the summary dictionary to a DataFrame\n",
    "    summary_df = pd.DataFrame(summary, index=[0])\n",
    "\n",
    "    # Create the directory to save the summary file\n",
    "    summary_folder = os.path.join(save_dir, \"summary\", run_tag, name)\n",
    "    os.makedirs(summary_folder, exist_ok=True)\n",
    "\n",
    "    # Define the path to save the summary CSV file\n",
    "    summary_path = os.path.join(summary_folder, f\"{name}_{prefix}_summary.csv\")\n",
    "    \n",
    "    # Save the summary DataFrame to CSV\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    print(f\"Saved summary metrics for {name} ({prefix}) to {summary_path}\")\n",
    "\n",
    "def psd(input_dir, run_tag, names, save_dir, save=True):\n",
    "    \"\"\"Analyze 3D instance segmentation and compute particle size distribution (PSD).\"\"\"\n",
    "    for name in names:\n",
    "        segmentation = load_data_from_dir(os.path.join(input_dir, name))\n",
    "        raw_psd_metrics = compute_psd(segmentation)\n",
    "\n",
    "        if save:\n",
    "            table_folder = os.path.join(save_dir, \"table\", run_tag, name)\n",
    "            hist_folder = os.path.join(save_dir, \"histogram\", run_tag, name)\n",
    "            os.makedirs(table_folder, exist_ok=True)\n",
    "            os.makedirs(hist_folder, exist_ok=True)\n",
    "\n",
    "            # Save original CSV\n",
    "            orignal_csv_path = os.path.join(table_folder, f\"{name}_raw_psd.csv\")\n",
    "            save_psd_to_csv(raw_psd_metrics, orignal_csv_path)\n",
    "\n",
    "            # Save IQR-filtered CSV\n",
    "            iqr_filt_psd_metrics = filter_by_iqr(raw_psd_metrics)\n",
    "            iqr_filt_csv_path = os.path.join(table_folder, f\"{name}_iqr_filt_psd.csv\")\n",
    "            save_psd_to_csv(iqr_filt_psd_metrics, iqr_filt_csv_path)\n",
    "\n",
    "            # Save threshold-filtered CSV\n",
    "            thresh_filt_psd_metrics, threshold = filter_by_threshold(raw_psd_metrics)\n",
    "            thresh_filt_csv_path = os.path.join(table_folder, f\"{name}_thresh_filt_psd.csv\")\n",
    "            save_psd_to_csv(thresh_filt_psd_metrics, thresh_filt_csv_path)\n",
    "\n",
    "            # Save binned PSD CSVs\n",
    "            raw_psd_bins = save_binned_psd(raw_psd_metrics, os.path.join(table_folder, f\"{name}_raw_binned_psd.csv\"), num_bins=75)\n",
    "            iqr_filt_psd_bins = save_binned_psd(iqr_filt_psd_metrics, os.path.join(table_folder, f\"{name}_iqr_filt_binned_psd.csv\"), num_bins=75)\n",
    "            thresh_filt_psd_bins = save_binned_psd(thresh_filt_psd_metrics, os.path.join(table_folder, f\"{name}_thresh_filt_binned_psd.csv\"), num_bins=75, threshold=threshold)\n",
    "\n",
    "            # Save histograms\n",
    "            psd_metric_names = ['Volume', 'Surface Area', 'Diameter', 'Feret Diameter', 'Solidity', 'Sphericity']\n",
    "            psd_metric_tags = ['volume', 'surface', 'diameter', 'feret', 'solidity', 'sphericity']\n",
    "            \n",
    "            # Raw PSD values\n",
    "            for raw_psd_metric, raw_psd_bin, psd_metric_name, psd_metric_tag in zip(raw_psd_metrics[1:], raw_psd_bins, psd_metric_names, psd_metric_tags):\n",
    "                if np.max(raw_psd_metric) <= 1:   \n",
    "                    title = f\"{psd_metric_name}\"\n",
    "                else:\n",
    "                    title = f\"{psd_metric_name} (voxels)\"\n",
    "                save_histogram(raw_psd_metric, raw_psd_bin, f\"Raw Particle {psd_metric_name} Distribution\", title, os.path.join(hist_folder, f\"{name}_raw_{psd_metric_tag}_hist.png\"))\n",
    "                \n",
    "            # IQR-filterd PSD values\n",
    "            for iqr_filt_psd_metric, iqr_filt_psd_bin, psd_metric_name, psd_metric_tag in zip(iqr_filt_psd_metrics[1:], iqr_filt_psd_bins, psd_metric_names, psd_metric_tags):\n",
    "                if np.max(iqr_filt_psd_metric) <= 1:   \n",
    "                    title = f\"{psd_metric_name}\"\n",
    "                else:\n",
    "                    title = f\"{psd_metric_name} (voxels)\"\n",
    "                save_histogram(iqr_filt_psd_metric, iqr_filt_psd_bin, f\"IQR-Filtered Particle {psd_metric_name} Distribution\", title, os.path.join(hist_folder, f\"{name}_raw_{psd_metric_tag}_hist.png\"))\n",
    "                \n",
    "            # Threshold-filterd PSD values\n",
    "            for thresh_filt_psd_metric, thresh_filt_psd_bin, psd_metric_name, psd_metric_tag in zip(thresh_filt_psd_metrics[1:], thresh_filt_psd_bins, psd_metric_names, psd_metric_tags):\n",
    "                if np.max(thresh_filt_psd_metric) <= 1:   \n",
    "                    title = f\"{psd_metric_name}\"\n",
    "                else:\n",
    "                    title = f\"{psd_metric_name} (voxels)\"\n",
    "                save_histogram(thresh_filt_psd_metric, thresh_filt_psd_bin, f\"IQR-Filtered Particle {psd_metric_name} Distribution\", title, os.path.join(hist_folder, f\"{name}_raw_{psd_metric_tag}_hist.png\"))\n",
    "\n",
    "            # Save summary metrics\n",
    "            lists_of_psd_metrics = [raw_psd_metrics[1:], iqr_filt_psd_metrics[1:], thresh_filt_psd_metrics[1:]]\n",
    "            prefixes = ['raw', 'iqr_filt', 'thresh_filt']\n",
    "            \n",
    "            for list_of_psd_metrics, prefix in zip(lists_of_psd_metrics, prefixes):\n",
    "                save_summary_metrics(list_of_psd_metrics, psd_metric_names, save_dir, run_tag, name, prefix=prefix)\n",
    "\n",
    "            print(f\"Saved results for {name}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_paths(dir_location, run_tag, output_to_cloud=False):\n",
    "    if dir_location.lower() == 'internal':\n",
    "        base_path = r'C:\\Senior_Design'\n",
    "    elif dir_location.lower() == 'external':\n",
    "        base_path = r'D:\\Senior_Design'\n",
    "    elif dir_location.lower() == 'cloud':\n",
    "        base_path = r'C:\\Users\\dchen\\OneDrive - University of Connecticut\\Courses\\Year 4\\Fall 2024\\BME 4900 and 4910W (Kumavor)\\Python\\Files'\n",
    "    elif dir_location.lower() == 'refine':\n",
    "        base_path = r'D:\\Darren\\Files'\n",
    "    else:\n",
    "        raise ValueError('Invalid directory location type')\n",
    "\n",
    "    base_input_path = os.path.join(base_path, 'outputs', 'tiff')\n",
    "    psd_path = os.path.join(base_path, 'outputs', 'metrics', 'particle_size_dist_v3')\n",
    "    \n",
    "    if output_to_cloud:\n",
    "        psd_path = os.path.join(r'C:\\Users\\dchen\\OneDrive - University of Connecticut\\Courses\\Year 4\\Fall 2024\\BME 4900 and 4910W (Kumavor)\\Python\\Files', 'outputs', 'metrics', 'particle_size_dist')\n",
    "\n",
    "    input_tiff_path = os.path.join(base_input_path, run_tag)\n",
    "\n",
    "    print('Paths set')\n",
    "    print('Input:', input_tiff_path)\n",
    "    print('Output:', psd_path)\n",
    "    print()\n",
    "    return input_tiff_path, psd_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_location = 'refine'\n",
    "run_tag = 'pretrained_tab40_gen35_clar35_fold4'\n",
    "input_tiff_path, psd_path = setup_paths(dir_location, run_tag)\n",
    "save = True  # Set to True if you want to save both histograms and PSD data\n",
    "names = ['2_Tablet', '4_GenericD12', '5_ClaritinD12']\n",
    "\n",
    "# For ground truth instance segmentation\n",
    "# input_tiff_path = r'C:\\Users\\dchen\\OneDrive - University of Connecticut\\Courses\\Year 4\\Fall 2024\\BME 4900 and 4910W (Kumavor)\\Python\\Files\\database\\tablet_dataset\\instance\\tiff'\n",
    "input_tiff_path = r'd:\\Darren\\Files\\database\\tablet_dataset\\instance\\tiff'\n",
    "# input_tiff_path = r'd:\\Senior_Design\\database\\tablet_dataset\\instance\\tiff'\n",
    "run_tag = 'ground_truth'\n",
    "\n",
    "psd(input_tiff_path, run_tag, names, psd_path, save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Senior_Design_py310_source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
