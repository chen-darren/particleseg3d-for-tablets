{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "import json\n",
    "from os.path import join\n",
    "from instance2border_core import instance2border_core\n",
    "import zarr\n",
    "from acvl_utils.miscellaneous.ptqdm import ptqdm\n",
    "import pickle\n",
    "from skimage.measure import regionprops\n",
    "import argparse\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "\n",
    "def preprocess_all(load_dir: str, names: List[str], save_dir: str, target_spacing: float,\n",
    "                   target_particle_size_in_pixel: int, dataset_name: str, processes: int,\n",
    "                   border_thickness_in_pixel: int, gpu: bool, bordercore: bool, save_zarr: bool) -> None:\n",
    "    \"\"\"\n",
    "    Preprocesses all the samples in the dataset.\n",
    "\n",
    "    :param load_dir: Path to the base directory that contains the dataset structured in the form of the directories 'images' and 'instance_seg' and the files metadata.json.\n",
    "    :param names: The name(s) without extension of the image(s) that should be used for training.\n",
    "    :param save_dir: Path to the preprocessed dataset directory.\n",
    "    :param target_spacing: The target spacing in millimeters.\n",
    "    :param target_particle_size_in_pixel: The target particle size in pixels.\n",
    "    :param dataset_name: The name of the preprocessed dataset.\n",
    "    :param processes: Number of processes to use for parallel processing. None to disable multiprocessing.\n",
    "    :param border_thickness_in_pixel: Border thickness in pixel.\n",
    "    :param gpu: Flag indicating whether to use the GPU for preprocessing.\n",
    "    :param bordercore: Flag indicating whether to border-core representation is used instead of instance segmentation.\n",
    "    :param save_zarr: Flag indicating whether to save zarr files.\n",
    "    \"\"\"\n",
    "    metadata_load_filepath = join(load_dir, \"metadata.json\")\n",
    "\n",
    "    with open(metadata_load_filepath) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    image_save_dir = join(save_dir, dataset_name, \"imagesTr\")\n",
    "    semantic_seg_save_dir = join(save_dir, dataset_name, \"labelsTr\")\n",
    "    instance_seg_save_dir = join(save_dir, dataset_name, \"labelsTr_instance\")\n",
    "    semantic_seg_zarr_save_dir = join(save_dir, dataset_name, \"labelsTr_zarr\")\n",
    "    instance_seg_zarr_save_dir = join(save_dir, dataset_name, \"labelsTr_instance_zarr\")\n",
    "    if not bordercore:\n",
    "        Path(instance_seg_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(image_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(semantic_seg_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for name in names:\n",
    "        if name not in metadata:\n",
    "            raise RuntimeError(\"{} is missing in metadata!\".format(name))\n",
    "\n",
    "    image_load_filepaths = [join(load_dir, \"images\", name + \".nii.gz\") for name in names]\n",
    "    if bordercore:\n",
    "        seg_load_filepaths = [join(load_dir, \"border_core\", name + \".nii.gz\") for name in names]\n",
    "    else:\n",
    "        seg_load_filepaths = [join(load_dir, \"instance_seg\", name + \".nii.gz\") for name in names]\n",
    "\n",
    "    if processes is None:\n",
    "        for i in tqdm(range(len(names))):\n",
    "            preprocess_single(i, names=names, image_load_filepaths=image_load_filepaths, seg_load_filepaths=seg_load_filepaths,\n",
    "                      metadata_load_filepath=metadata_load_filepath, image_save_dir=image_save_dir,\n",
    "                      semantic_seg_save_dir=semantic_seg_save_dir, instance_seg_save_dir=instance_seg_save_dir,\n",
    "                      semantic_seg_zarr_save_dir=semantic_seg_zarr_save_dir, instance_seg_zarr_save_dir=instance_seg_zarr_save_dir,\n",
    "                      target_spacing=[target_spacing], target_particle_size_in_pixel=[target_particle_size_in_pixel],\n",
    "                      border_thickness_in_pixel=border_thickness_in_pixel, gpu=gpu, bordercore=bordercore, save_zarr=save_zarr)\n",
    "    else:\n",
    "        ptqdm(preprocess_single, range(len(names)), processes, names=names, image_load_filepaths=image_load_filepaths,\n",
    "                  seg_load_filepaths=seg_load_filepaths, metadata_load_filepath=metadata_load_filepath,\n",
    "                  image_save_dir=image_save_dir, semantic_seg_save_dir=semantic_seg_save_dir,\n",
    "                  instance_seg_save_dir=instance_seg_save_dir, semantic_seg_zarr_save_dir=semantic_seg_zarr_save_dir,\n",
    "                  instance_seg_zarr_save_dir=instance_seg_zarr_save_dir, target_spacing=[target_spacing],\n",
    "                  target_particle_size_in_pixel=[target_particle_size_in_pixel],\n",
    "                  border_thickness_in_pixel=border_thickness_in_pixel, gpu=gpu, bordercore=bordercore, save_zarr=save_zarr)\n",
    "\n",
    "    utils.generate_dataset_json(join(save_dir, dataset_name, 'dataset.json'), join(save_dir, dataset_name, \"imagesTr\"), None, (\"noNorm\",), {0: 'bg', 1: 'core', 2: 'border'}, dataset_name)\n",
    "\n",
    "    if not bordercore:\n",
    "        gen_regionprops(join(save_dir, dataset_name, \"labelsTr_instance\"), join(save_dir, dataset_name, \"regionprops.pkl\"))\n",
    "\n",
    "\n",
    "def preprocess_single(i: int,\n",
    "                      names: List[str],\n",
    "                      image_load_filepaths: List[str],\n",
    "                      seg_load_filepaths: List[str],\n",
    "                      metadata_load_filepath: str,\n",
    "                      image_save_dir: str,\n",
    "                      semantic_seg_save_dir: str,\n",
    "                      instance_seg_save_dir: str,\n",
    "                      semantic_seg_zarr_save_dir: str,\n",
    "                      instance_seg_zarr_save_dir: str,\n",
    "                      target_spacing: List[float],\n",
    "                      target_particle_size_in_pixel: List[int],\n",
    "                      border_thickness_in_pixel: int,\n",
    "                      gpu: bool,\n",
    "                      bordercore: bool,\n",
    "                      save_zarr: bool) -> None:\n",
    "    \"\"\"\n",
    "    Preprocess a single 3D particle segmentation image.\n",
    "\n",
    "    Args:\n",
    "        i (int): Index of the image to preprocess.\n",
    "        names (List[str]): Names of the images to preprocess.\n",
    "        image_load_filepaths (List[str]): Paths to the input 3D particle segmentation image files.\n",
    "        seg_load_filepaths (List[str]): Paths to the input instance segmentation or border-core representation image files.\n",
    "        metadata_load_filepath (str): Path to the metadata file.\n",
    "        image_save_dir (str): Path to the directory to save the preprocessed images.\n",
    "        semantic_seg_save_dir (str): Path to the directory to save the semantic segmentation images.\n",
    "        instance_seg_save_dir (str): Path to the directory to save the instance segmentation images.\n",
    "        semantic_seg_zarr_save_dir (str): Path to the directory to save the semantic segmentation images in zarr format.\n",
    "        instance_seg_zarr_save_dir (str): Path to the directory to save the instance segmentation images in zarr format.\n",
    "        target_spacing (List[float]): Target spacing in millimeters.\n",
    "        target_particle_size_in_pixel (List[int]): Target particle size in pixels.\n",
    "        border_thickness_in_pixel (int): Border thickness in pixels.\n",
    "        gpu (bool): If True, use GPU for resampling.\n",
    "        bordercore (bool): If True, border-core representation is used instead of instance segmentation.\n",
    "        save_zarr (bool): If True, saves all Zarr files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    name = names[i]\n",
    "    image_load_filepath = image_load_filepaths[i]\n",
    "    seg_load_filepath = seg_load_filepaths[i]\n",
    "\n",
    "    with open(metadata_load_filepath) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    image = utils.load_nifti(image_load_filepath)\n",
    "\n",
    "    # zscore = {\"mean\": zscore[0], \"std\": zscore[1]}\n",
    "    zscore = {\"mean\": np.mean(image), \"std\": np.std(image)} # Automated Z-Scoring per image\n",
    "    image = utils.standardize(image, zscore)\n",
    "\n",
    "    image_shape = image.shape\n",
    "    source_particle_size_in_mm = [metadata[name][\"particle_size\"]] * 3\n",
    "    source_spacing = [metadata[name][\"spacing\"]] * 3\n",
    "    target_particle_size_in_mm = tuple(utils.pixel2mm(target_particle_size_in_pixel, target_spacing))\n",
    "\n",
    "    size_conversion_factor = utils.compute_size_conversion_factor(source_particle_size_in_mm, source_spacing, target_particle_size_in_mm, target_spacing)\n",
    "    target_patch_size_in_pixel = np.rint(np.asarray(image_shape) / size_conversion_factor).astype(int)\n",
    "    target_patch_size_in_pixel = target_patch_size_in_pixel.tolist()\n",
    "\n",
    "    image = utils.resample(image, target_patch_size_in_pixel, gpu=gpu, disable=True)\n",
    "    patch_name = \"{}\".format(name)\n",
    "    image_save_filepath = join(image_save_dir, patch_name + \"_0000.nii.gz\")\n",
    "    utils.save_nifti(image_save_filepath, image, spacing=target_spacing*3)\n",
    "\n",
    "    if bordercore:\n",
    "        semantic_seg = utils.load_nifti(seg_load_filepath)\n",
    "        semantic_seg = utils.resample(semantic_seg, target_patch_size_in_pixel, gpu=gpu, seg=True, disable=True)\n",
    "    else:\n",
    "        instance_seg = utils.load_nifti(seg_load_filepath)\n",
    "        instance_seg = utils.resample(instance_seg, target_patch_size_in_pixel, gpu=gpu, seg=True, disable=True)\n",
    "        instance_seg_save_filepath = join(instance_seg_save_dir, patch_name + \".nii.gz\")\n",
    "        utils.save_nifti(instance_seg_save_filepath, instance_seg, spacing=target_spacing, is_seg=True, dtype=np.uint16)\n",
    "        semantic_seg = instance2border_core(instance_seg, border_thickness_in_pixel)\n",
    "\n",
    "    semantic_seg_save_filepath = join(semantic_seg_save_dir, patch_name + \".nii.gz\")\n",
    "    utils.save_nifti(semantic_seg_save_filepath, semantic_seg, spacing=target_spacing, is_seg=True, dtype=np.uint8)\n",
    "    \n",
    "    if save_zarr:\n",
    "        semantic_seg_zarr_save_filepath = join(semantic_seg_zarr_save_dir, patch_name + \".zarr\")\n",
    "        semantic_seg = zarr.array(semantic_seg)\n",
    "        zarr.save(semantic_seg_zarr_save_filepath, semantic_seg, chunks=(64, 64, 64))\n",
    "        if not bordercore:\n",
    "            instance_seg_zarr_save_filepath = join(instance_seg_zarr_save_dir, patch_name + \".zarr\")\n",
    "            instance_seg = zarr.array(instance_seg)\n",
    "            zarr.save(instance_seg_zarr_save_filepath, instance_seg, chunks=(64, 64, 64))\n",
    "\n",
    "\n",
    "def gen_regionprops(load_dir: str, metadata_filepath: str) -> None:\n",
    "    \"\"\"Extracts regionprops features from the given instance segmentation data and saves the data to the given file.\n",
    "\n",
    "    Args:\n",
    "        load_dir (str): Absolute path to the directory containing instance segmentation data.\n",
    "        metadata_filepath (str): Absolute path to the file where the extracted regionprops features should be saved.\n",
    "    \"\"\"\n",
    "    names = utils.load_filepaths(load_dir, return_path=False, return_extension=False)\n",
    "    metadata = {}\n",
    "\n",
    "    len_props_total, len_props_filtered_total = 0, 0\n",
    "    for name in tqdm(names):\n",
    "        instance_seg = utils.load_nifti(join(load_dir, name + \".nii.gz\"))\n",
    "        props, len_props, len_props_filtered = gen_regionprops_single(instance_seg)\n",
    "        len_props_total += len_props\n",
    "        len_props_filtered_total += len_props_filtered\n",
    "        metadata[name] = props\n",
    "\n",
    "    with open(metadata_filepath, 'wb') as handle:\n",
    "        pickle.dump(metadata, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def gen_regionprops_single(instance_seg: np.ndarray) -> Tuple[Dict[int, Tuple[int, int, int, int, int, int]], int, int]:\n",
    "    \"\"\"Extracts regionprops features from a single instance segmentation volume.\n",
    "\n",
    "    Args:\n",
    "        instance_seg (np.ndarray): A 3D numpy array containing the instance segmentation data.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the extracted regionprops features, the number of total regionprops, and the number of filtered regionprops.\n",
    "    \"\"\"\n",
    "    props = {prop.label: prop.bbox for prop in regionprops(instance_seg)}\n",
    "\n",
    "    props_filtered = {}\n",
    "    image_shape = instance_seg.shape\n",
    "    for label, bbox in props.items():\n",
    "        bbox_reshaped = [[bbox[i], bbox[i + len(bbox) // 2]] for i in range(len(bbox) // 2)]\n",
    "        bbox_reshaped = np.asarray(bbox_reshaped)\n",
    "        ok = True\n",
    "        for axis in range(len(image_shape)):\n",
    "            if bbox_reshaped[axis][0] == 0 or bbox_reshaped[axis][1] == image_shape[axis]:\n",
    "                ok = False\n",
    "        if ok:\n",
    "            props_filtered[label] = bbox\n",
    "\n",
    "    return props_filtered, len(props), len(props_filtered)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-i', \"--input\", required=True,\n",
    "#                         help=\"Absolute input path to the base folder that contains the dataset structured in the form of the directories 'images' and 'instance_seg' and the file metadata.json.\")\n",
    "#     parser.add_argument('-o', \"--output\", required=True, help=\"Absolute output path to the preprocessed dataset directory.\")\n",
    "#     parser.add_argument('-n', \"--name\", required=False, type=str, default=None, nargs=\"+\", help=\"(Optional) The name(s) without extension of the image(s) that should be used for training. Multiple names must be separated by spaces.\")\n",
    "#     parser.add_argument('-t', '--task', required=False, default=500, type=int, help=\"(Optional) The task id that should be assigned to this dataset.\")\n",
    "#     parser.add_argument('-z', '--zscore', default=(5850.29762143569, 7078.294543817302), required=False, type=float, nargs=2,\n",
    "#                         help=\"(Optional) The target spacing in millimeters given as three numbers separate by spaces.\")\n",
    "#     parser.add_argument('-target_particle_size', default=60, required=False, type=int,\n",
    "#                         help=\"(Optional) The target particle size in pixels given as three numbers separate by spaces.\")\n",
    "#     parser.add_argument('-target_spacing', default=0.1, required=False, type=float,\n",
    "#                         help=\"(Optional) The target spacing in millimeters given as three numbers separate by spaces.\")\n",
    "#     parser.add_argument('-p', '--processes', required=False, default=None, type=int, help=\"(Optional) Number of processes to use for parallel processing. None to disable multiprocessing.\")\n",
    "#     parser.add_argument('-thickness', required=False, default=2, type=int, help=\"(Optional) Border thickness in pixel.\")\n",
    "#     parser.add_argument('--disable_gpu', required=False, default=False, action=\"store_true\", help=\"(Optional) Disables use of the GPU for preprocessing.\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"Preprocess a dataset for training a particle segmentation model.\")\n",
    "\n",
    "#     names = args.name\n",
    "\n",
    "#     if names is None:\n",
    "#         names = utils.load_filepaths(join(args.input, \"images\"), extension=\".nii.gz\", return_path=False, return_extension=False)\n",
    "\n",
    "#     print(\"Samples: \", names)\n",
    "#     print(\"Num samples: \", len(names))\n",
    "\n",
    "#     dataset_name = \"Task{}_ParticleSeg3D\".format(str(args.task).zfill(3))\n",
    "\n",
    "#     preprocess_all(args.input, names, args.output, args.target_spacing, args.target_particle_size, dataset_name, args.processes, args.thickness, not args.disable_gpu, args.zscore)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def setup_paths(dir_location, is_original_data, task):\n",
    "    if dir_location.lower() == 'internal':\n",
    "        base_path = r'C:\\Senior_Design'\n",
    "    elif dir_location.lower() == 'external':\n",
    "        base_path = r'D:\\Senior_Design'\n",
    "    elif dir_location.lower() == 'cloud':\n",
    "        base_path = r'C:\\Users\\dchen\\OneDrive - University of Connecticut\\Courses\\Year 4\\Fall 2024\\BME 4900 and 4910W (Kumavor)\\Python\\Files'\n",
    "    elif dir_location.lower() == 'refine':\n",
    "        base_path = r'D:\\Darren\\Files'\n",
    "    else:\n",
    "        raise ValueError('Invalid directory location type')\n",
    "    \n",
    "    base_input_path = os.path.join(base_path, 'database')\n",
    "    if is_original_data:\n",
    "        input_path = os.path.join(base_input_path, 'orignal_dataset', 'training', 'Task' + str(task))\n",
    "    else:\n",
    "        input_path = os.path.join(base_input_path, 'tablet_dataset', 'training', 'Task' + str(task))\n",
    "    \n",
    "    if not os.path.isdir(input_path):\n",
    "        raise ValueError(\"This input path is not valid:\\n\" + input_path)\n",
    "\n",
    "    output_path = os.path.join(base_path, 'training', 'nnUNet_preprocessed')\n",
    "    if not os.path.isdir(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    print('Paths set')\n",
    "    return input_path, output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set\n",
      "Samples:  ['2_Tablet_Aug1', '2_Tablet_Aug2', '2_Tablet_Aug3', '2_Tablet_Aug4', '2_Tablet_Aug5', '4_GenericD12_Aug1', '4_GenericD12_Aug2', '4_GenericD12_Aug3', '4_GenericD12_Aug4', '4_GenericD12_Aug5', '5_ClaritinD12_Aug1', '5_ClaritinD12_Aug2', '5_ClaritinD12_Aug3', '5_ClaritinD12_Aug4', '5_ClaritinD12_Aug5']\n",
      "Num samples:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: About to print spacing\n",
      "[0.1]\n",
      "[0.1, 0.1, 0.1]\n",
      "DEBUG: Spacing printed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown in SimpleITK Image_SetSpacing: D:\\a\\SimpleITK\\SimpleITK\\Code\\Common\\include\\sitkTemplateFunctions.h:103:\nsitk::ERROR: Unable to convert vector to ITK type\nExpected vector of length 3 but only got 1 elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21828\\784309107.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num samples: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Task{}_ParticleSeg3D\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mpreprocess_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_spacing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_particle_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdisable_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbordercore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_zarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21828\\4289885646.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(load_dir, names, save_dir, target_spacing, target_particle_size_in_pixel, dataset_name, processes, border_thickness_in_pixel, gpu, bordercore, save_zarr)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mseg_load_filepaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"instance_seg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".nii.gz\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             preprocess_single(i, names=names, image_load_filepaths=image_load_filepaths, seg_load_filepaths=seg_load_filepaths,\n\u001b[0m\u001b[0;32m     62\u001b[0m                       \u001b[0mmetadata_load_filepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata_load_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_save_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_save_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                       \u001b[0msemantic_seg_save_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msemantic_seg_save_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance_seg_save_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstance_seg_save_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                       \u001b[0msemantic_seg_zarr_save_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msemantic_seg_zarr_save_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance_seg_zarr_save_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstance_seg_zarr_save_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21828\\4289885646.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(i, names, image_load_filepaths, seg_load_filepaths, metadata_load_filepath, image_save_dir, semantic_seg_save_dir, instance_seg_save_dir, semantic_seg_zarr_save_dir, instance_seg_zarr_save_dir, target_spacing, target_particle_size_in_pixel, border_thickness_in_pixel, gpu, bordercore, save_zarr)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DEBUG: About to print spacing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_spacing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_spacing\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DEBUG: Spacing printed successfully\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_nifti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_save_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspacing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_spacing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbordercore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0msemantic_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_nifti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_load_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Darren\\Senior_Design\\particleseg3d_source\\train\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(filename, image, spacing, affine, header, is_seg, dtype)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetMetaData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspacing\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m         \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetSpacing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspacing\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maffine\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# How do I set the affine transform with SimpleITK? With NiBabel it is just nib.Nifti1Image(img, affine=affine, header=header)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ZEISS\\anaconda3\\envs\\Senior_Design_py310_source\\lib\\site-packages\\SimpleITK\\SimpleITK.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, spacing)\u001b[0m\n\u001b[0;32m   3248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mSetSpacing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspacing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3249\u001b[0m         \u001b[1;34mr\"\"\"SetSpacing(Image self, VectorDouble spacing)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3250\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_SimpleITK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage_SetSpacing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspacing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception thrown in SimpleITK Image_SetSpacing: D:\\a\\SimpleITK\\SimpleITK\\Code\\Common\\include\\sitkTemplateFunctions.h:103:\nsitk::ERROR: Unable to convert vector to ITK type\nExpected vector of length 3 but only got 1 elements."
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from os.path import join\n",
    "\n",
    "# Set argument values directly\n",
    "input_path = \"/absolute/path/to/dataset\"  # Absolute path to the directory containing 'metadata.json', 'images', and either 'instance_seg' or 'border_core' directories\n",
    "output_path = \"/absolute/path/to/output\"  # Absolute path where the preprocessed dataset will be stored\n",
    "names = None  # List of image names (without extensions) to use for training; None to use all available images\n",
    "task = '500'  # Task ID (int) assigned to the dataset\n",
    "target_spacing = 0.1  # Target spacing in millimeters\n",
    "target_particle_size = 60  # Target particle size in pixels\n",
    "processes = None  # Number of processes for parallel processing; None disables multiprocessing\n",
    "thickness = 2  # Border thickness in pixels (erosion only)\n",
    "disable_gpu = False  # If True, disables GPU usage for preprocessing\n",
    "bordercore = False # Used if the border-core representation is used instead of instance segmentation.\n",
    "save_zarr = False # Used to determine if Zarr files should be saved.\n",
    "\n",
    "# Paths\n",
    "input_path, output_path = setup_paths('refine', False, task)\n",
    "\n",
    "if names is None:\n",
    "    names = utils.load_filepaths(join(input_path, \"images\"), extension=\".nii.gz\", return_path=False, return_extension=False)\n",
    "\n",
    "print(\"Samples: \", names)\n",
    "print(\"Num samples: \", len(names))\n",
    "\n",
    "dataset_name = \"Task{}_ParticleSeg3D\".format(str(task).zfill(3))\n",
    "\n",
    "preprocess_all(input_path, names, output_path, target_spacing, target_particle_size, dataset_name, processes, thickness, not disable_gpu, bordercore, save_zarr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from os.path import join\n",
    "\n",
    "# Set argument values directly\n",
    "input_path = \"/absolute/path/to/dataset\"  # Absolute path to the directory containing 'metadata.json', 'images', and either 'instance_seg' or 'border_core' directories\n",
    "output_path = \"/absolute/path/to/output\"  # Absolute path where the preprocessed dataset will be stored\n",
    "names = None  # List of image names (without extensions) to use for training; None to use all available images\n",
    "task = '501'  # Task ID (int) assigned to the dataset\n",
    "target_spacing = 0.1  # Target spacing in millimeters\n",
    "target_particle_size = 60  # Target particle size in pixels\n",
    "processes = None  # Number of processes for parallel processing; None disables multiprocessing\n",
    "thickness = 2  # Border thickness in pixels (erosion only)\n",
    "disable_gpu = False  # If True, disables GPU usage for preprocessing\n",
    "bordercore = True # Used if the border-core representation is used instead of instance segmentation.\n",
    "save_zarr = False # Used to determine if Zarr files should be saved.\n",
    "\n",
    "# Paths\n",
    "input_path, output_path = setup_paths('refine', False, task)\n",
    "\n",
    "if names is None:\n",
    "    names = utils.load_filepaths(join(input_path, \"images\"), extension=\".nii.gz\", return_path=False, return_extension=False)\n",
    "\n",
    "print(\"Samples: \", names)\n",
    "print(\"Num samples: \", len(names))\n",
    "\n",
    "dataset_name = \"Task{}_ParticleSeg3D\".format(str(task).zfill(3))\n",
    "\n",
    "preprocess_all(input_path, names, output_path, target_spacing, target_particle_size, dataset_name, processes, thickness, not disable_gpu, bordercore, save_zarr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Senior_Design_py310_source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
