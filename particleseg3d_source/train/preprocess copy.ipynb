{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "import json\n",
    "from os.path import join\n",
    "from instance2border_core import instance2border_core\n",
    "import zarr\n",
    "from acvl_utils.miscellaneous.ptqdm import ptqdm\n",
    "import pickle\n",
    "from skimage.measure import regionprops\n",
    "import argparse\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "\n",
    "def preprocess_all(load_dir: str, names: List[str], save_dir: str, target_spacing: float,\n",
    "                   target_particle_size_in_pixel: int, dataset_name: str, processes: int,\n",
    "                   border_thickness_in_pixel: int, gpu: bool, bordercore: bool, save_zarr: bool, device: int) -> None:\n",
    "    \"\"\"\n",
    "    Preprocesses all the samples in the dataset.\n",
    "\n",
    "    :param load_dir: Path to the base directory that contains the dataset structured in the form of the directories 'images' and 'instance_seg' and the files metadata.json.\n",
    "    :param names: The name(s) without extension of the image(s) that should be used for training.\n",
    "    :param save_dir: Path to the preprocessed dataset directory.\n",
    "    :param target_spacing: The target spacing in millimeters.\n",
    "    :param target_particle_size_in_pixel: The target particle size in pixels.\n",
    "    :param dataset_name: The name of the preprocessed dataset.\n",
    "    :param processes: Number of processes to use for parallel processing. None to disable multiprocessing.\n",
    "    :param border_thickness_in_pixel: Border thickness in pixel.\n",
    "    :param gpu: Flag indicating whether to use the GPU for preprocessing.\n",
    "    :param bordercore: Flag indicating whether to border-core representation is used instead of instance segmentation.\n",
    "    :param save_zarr: Flag indicating whether to save zarr files.\n",
    "    :param device: Value indicating which GPU to use (0 or 1 only)\n",
    "    \"\"\"\n",
    "    metadata_load_filepath = join(load_dir, \"metadata.json\")\n",
    "\n",
    "    with open(metadata_load_filepath) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    target_spacing = [target_spacing] * 3\n",
    "    target_particle_size_in_pixel = [target_particle_size_in_pixel] * 3\n",
    "\n",
    "    image_save_dir = join(save_dir, dataset_name, \"imagesTr\")\n",
    "    semantic_seg_save_dir = join(save_dir, dataset_name, \"labelsTr\")\n",
    "    instance_seg_save_dir = join(save_dir, dataset_name, \"labelsTr_instance\")\n",
    "    semantic_seg_zarr_save_dir = join(save_dir, dataset_name, \"labelsTr_zarr\")\n",
    "    instance_seg_zarr_save_dir = join(save_dir, dataset_name, \"labelsTr_instance_zarr\")\n",
    "    if not bordercore:\n",
    "        Path(instance_seg_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(image_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(semantic_seg_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for name in names:\n",
    "        if name not in metadata:\n",
    "            raise RuntimeError(\"{} is missing in metadata!\".format(name))\n",
    "\n",
    "    image_load_filepaths = [join(load_dir, \"images\", name + \".nii.gz\") for name in names]\n",
    "    if bordercore:\n",
    "        seg_load_filepaths = [join(load_dir, \"border_core\", name + \".nii.gz\") for name in names]\n",
    "    else:\n",
    "        seg_load_filepaths = [join(load_dir, \"instance_seg\", name + \".nii.gz\") for name in names]\n",
    "\n",
    "    if processes is None:\n",
    "        for i in tqdm(range(len(names))):\n",
    "            preprocess_single(i, names=names, image_load_filepaths=image_load_filepaths, seg_load_filepaths=seg_load_filepaths,\n",
    "                      metadata_load_filepath=metadata_load_filepath, image_save_dir=image_save_dir,\n",
    "                      semantic_seg_save_dir=semantic_seg_save_dir, instance_seg_save_dir=instance_seg_save_dir,\n",
    "                      semantic_seg_zarr_save_dir=semantic_seg_zarr_save_dir, instance_seg_zarr_save_dir=instance_seg_zarr_save_dir,\n",
    "                      target_spacing=target_spacing, target_particle_size_in_pixel=target_particle_size_in_pixel,\n",
    "                      border_thickness_in_pixel=border_thickness_in_pixel, gpu=gpu, bordercore=bordercore, save_zarr=save_zarr, device=device)\n",
    "    else:\n",
    "        ptqdm(preprocess_single, range(len(names)), processes, names=names, image_load_filepaths=image_load_filepaths,\n",
    "                  seg_load_filepaths=seg_load_filepaths, metadata_load_filepath=metadata_load_filepath,\n",
    "                  image_save_dir=image_save_dir, semantic_seg_save_dir=semantic_seg_save_dir,\n",
    "                  instance_seg_save_dir=instance_seg_save_dir, semantic_seg_zarr_save_dir=semantic_seg_zarr_save_dir,\n",
    "                  instance_seg_zarr_save_dir=instance_seg_zarr_save_dir, target_spacing=target_spacing,\n",
    "                  target_particle_size_in_pixel=target_particle_size_in_pixel,\n",
    "                  border_thickness_in_pixel=border_thickness_in_pixel, gpu=gpu, bordercore=bordercore, save_zarr=save_zarr, device=device)\n",
    "\n",
    "    utils.generate_dataset_json(join(save_dir, dataset_name, 'dataset.json'), join(save_dir, dataset_name, \"imagesTr\"), None, (\"noNorm\",), {0: 'bg', 1: 'core', 2: 'border'}, dataset_name)\n",
    "\n",
    "    if not bordercore:\n",
    "        gen_regionprops(join(save_dir, dataset_name, \"labelsTr_instance\"), join(save_dir, dataset_name, \"regionprops.pkl\"))\n",
    "\n",
    "\n",
    "def preprocess_single(i: int,\n",
    "                      names: List[str],\n",
    "                      image_load_filepaths: List[str],\n",
    "                      seg_load_filepaths: List[str],\n",
    "                      metadata_load_filepath: str,\n",
    "                      image_save_dir: str,\n",
    "                      semantic_seg_save_dir: str,\n",
    "                      instance_seg_save_dir: str,\n",
    "                      semantic_seg_zarr_save_dir: str,\n",
    "                      instance_seg_zarr_save_dir: str,\n",
    "                      target_spacing: List[float],\n",
    "                      target_particle_size_in_pixel: List[int],\n",
    "                      border_thickness_in_pixel: int,\n",
    "                      gpu: bool,\n",
    "                      bordercore: bool,\n",
    "                      save_zarr: bool,\n",
    "                      device: int) -> None:\n",
    "    \"\"\"\n",
    "    Preprocess a single 3D particle segmentation image.\n",
    "\n",
    "    Args:\n",
    "        i (int): Index of the image to preprocess.\n",
    "        names (List[str]): Names of the images to preprocess.\n",
    "        image_load_filepaths (List[str]): Paths to the input 3D particle segmentation image files.\n",
    "        seg_load_filepaths (List[str]): Paths to the input instance segmentation or border-core representation image files.\n",
    "        metadata_load_filepath (str): Path to the metadata file.\n",
    "        image_save_dir (str): Path to the directory to save the preprocessed images.\n",
    "        semantic_seg_save_dir (str): Path to the directory to save the semantic segmentation images.\n",
    "        instance_seg_save_dir (str): Path to the directory to save the instance segmentation images.\n",
    "        semantic_seg_zarr_save_dir (str): Path to the directory to save the semantic segmentation images in zarr format.\n",
    "        instance_seg_zarr_save_dir (str): Path to the directory to save the instance segmentation images in zarr format.\n",
    "        target_spacing (List[float]): Target spacing in millimeters.\n",
    "        target_particle_size_in_pixel (List[int]): Target particle size in pixels.\n",
    "        border_thickness_in_pixel (int): Border thickness in pixels.\n",
    "        gpu (bool): If True, use GPU for resampling.\n",
    "        bordercore (bool): If True, border-core representation is used instead of instance segmentation.\n",
    "        save_zarr (bool): If True, saves all Zarr files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    name = names[i]\n",
    "    image_load_filepath = image_load_filepaths[i]\n",
    "    seg_load_filepath = seg_load_filepaths[i]\n",
    "\n",
    "    with open(metadata_load_filepath) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    image = utils.load_nifti(image_load_filepath)\n",
    "\n",
    "    # zscore = {\"mean\": zscore[0], \"std\": zscore[1]}\n",
    "    zscore = {\"mean\": np.mean(image), \"std\": np.std(image)} # Automated Z-Scoring per image\n",
    "    image = utils.standardize(image, zscore)\n",
    "\n",
    "    image_shape = image.shape\n",
    "    source_particle_size_in_mm = [metadata[name][\"particle_size\"]] * 3\n",
    "    source_spacing = [metadata[name][\"spacing\"]] * 3\n",
    "    target_particle_size_in_mm = tuple(utils.pixel2mm(target_particle_size_in_pixel, target_spacing))\n",
    "\n",
    "    size_conversion_factor = utils.compute_size_conversion_factor(source_particle_size_in_mm, source_spacing, target_particle_size_in_mm, target_spacing)\n",
    "    target_patch_size_in_pixel = np.rint(np.asarray(image_shape) / size_conversion_factor).astype(int)\n",
    "    target_patch_size_in_pixel = target_patch_size_in_pixel.tolist()\n",
    "\n",
    "    image = utils.resample(image, target_patch_size_in_pixel, gpu=gpu, disable=True, device=device)\n",
    "    patch_name = \"{}\".format(name)\n",
    "    image_save_filepath = join(image_save_dir, patch_name + \"_0000.nii.gz\")\n",
    "    utils.save_nifti(image_save_filepath, image, spacing=target_spacing) # Why not saved as np.uint16? Originally saving it as np.float32: I think it is because of the normalization which converts to float 3/8/25\n",
    "\n",
    "    if bordercore:\n",
    "        semantic_seg = utils.load_nifti(seg_load_filepath)\n",
    "        semantic_seg = utils.resample(semantic_seg, target_patch_size_in_pixel, gpu=gpu, seg=True, disable=True, device=device)\n",
    "    else:\n",
    "        instance_seg = utils.load_nifti(seg_load_filepath)\n",
    "        instance_seg = utils.resample(instance_seg, target_patch_size_in_pixel, gpu=gpu, seg=True, disable=True, device=device)\n",
    "        instance_seg_save_filepath = join(instance_seg_save_dir, patch_name + \".nii.gz\")\n",
    "        utils.save_nifti(instance_seg_save_filepath, instance_seg, spacing=target_spacing, is_seg=True)\n",
    "        semantic_seg = instance2border_core(instance_seg, border_thickness_in_pixel)\n",
    "\n",
    "    semantic_seg_save_filepath = join(semantic_seg_save_dir, patch_name + \".nii.gz\")\n",
    "    utils.save_nifti(semantic_seg_save_filepath, semantic_seg, spacing=target_spacing, is_seg=True, dtype=np.uint8)\n",
    "    \n",
    "    if save_zarr:\n",
    "        semantic_seg_zarr_save_filepath = join(semantic_seg_zarr_save_dir, patch_name + \".zarr\")\n",
    "        semantic_seg = zarr.array(semantic_seg)\n",
    "        zarr.save(semantic_seg_zarr_save_filepath, semantic_seg, chunks=(64, 64, 64))\n",
    "        if not bordercore:\n",
    "            instance_seg_zarr_save_filepath = join(instance_seg_zarr_save_dir, patch_name + \".zarr\")\n",
    "            instance_seg = zarr.array(instance_seg)\n",
    "            zarr.save(instance_seg_zarr_save_filepath, instance_seg, chunks=(64, 64, 64))\n",
    "\n",
    "\n",
    "def gen_regionprops(load_dir: str, metadata_filepath: str) -> None:\n",
    "    \"\"\"Extracts regionprops features from the given instance segmentation data and saves the data to the given file.\n",
    "\n",
    "    Args:\n",
    "        load_dir (str): Absolute path to the directory containing instance segmentation data.\n",
    "        metadata_filepath (str): Absolute path to the file where the extracted regionprops features should be saved.\n",
    "    \"\"\"\n",
    "    names = utils.load_filepaths(load_dir, return_path=False, return_extension=False)\n",
    "    metadata = {}\n",
    "\n",
    "    len_props_total, len_props_filtered_total = 0, 0\n",
    "    for name in tqdm(names):\n",
    "        instance_seg = utils.load_nifti(join(load_dir, name + \".nii.gz\"))\n",
    "        props, len_props, len_props_filtered = gen_regionprops_single(instance_seg)\n",
    "        len_props_total += len_props\n",
    "        len_props_filtered_total += len_props_filtered\n",
    "        metadata[name] = props\n",
    "\n",
    "    with open(metadata_filepath, 'wb') as handle:\n",
    "        pickle.dump(metadata, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def gen_regionprops_single(instance_seg: np.ndarray) -> Tuple[Dict[int, Tuple[int, int, int, int, int, int]], int, int]:\n",
    "    \"\"\"Extracts regionprops features from a single instance segmentation volume.\n",
    "\n",
    "    Args:\n",
    "        instance_seg (np.ndarray): A 3D numpy array containing the instance segmentation data.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the extracted regionprops features, the number of total regionprops, and the number of filtered regionprops.\n",
    "    \"\"\"\n",
    "    props = {prop.label: prop.bbox for prop in regionprops(instance_seg)}\n",
    "\n",
    "    props_filtered = {}\n",
    "    image_shape = instance_seg.shape\n",
    "    for label, bbox in props.items():\n",
    "        bbox_reshaped = [[bbox[i], bbox[i + len(bbox) // 2]] for i in range(len(bbox) // 2)]\n",
    "        bbox_reshaped = np.asarray(bbox_reshaped)\n",
    "        ok = True\n",
    "        for axis in range(len(image_shape)):\n",
    "            if bbox_reshaped[axis][0] == 0 or bbox_reshaped[axis][1] == image_shape[axis]:\n",
    "                ok = False\n",
    "        if ok:\n",
    "            props_filtered[label] = bbox\n",
    "\n",
    "    return props_filtered, len(props), len(props_filtered)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-i', \"--input\", required=True,\n",
    "#                         help=\"Absolute input path to the base folder that contains the dataset structured in the form of the directories 'images' and 'instance_seg' and the file metadata.json.\")\n",
    "#     parser.add_argument('-o', \"--output\", required=True, help=\"Absolute output path to the preprocessed dataset directory.\")\n",
    "#     parser.add_argument('-n', \"--name\", required=False, type=str, default=None, nargs=\"+\", help=\"(Optional) The name(s) without extension of the image(s) that should be used for training. Multiple names must be separated by spaces.\")\n",
    "#     parser.add_argument('-t', '--task', required=False, default=500, type=int, help=\"(Optional) The task id that should be assigned to this dataset.\")\n",
    "#     parser.add_argument('-z', '--zscore', default=(5850.29762143569, 7078.294543817302), required=False, type=float, nargs=2,\n",
    "#                         help=\"(Optional) The target spacing in millimeters given as three numbers separate by spaces.\")\n",
    "#     parser.add_argument('-target_particle_size', default=60, required=False, type=int,\n",
    "#                         help=\"(Optional) The target particle size in pixels given as three numbers separate by spaces.\")\n",
    "#     parser.add_argument('-target_spacing', default=0.1, required=False, type=float,\n",
    "#                         help=\"(Optional) The target spacing in millimeters given as three numbers separate by spaces.\")\n",
    "#     parser.add_argument('-p', '--processes', required=False, default=None, type=int, help=\"(Optional) Number of processes to use for parallel processing. None to disable multiprocessing.\")\n",
    "#     parser.add_argument('-thickness', required=False, default=2, type=int, help=\"(Optional) Border thickness in pixel.\")\n",
    "#     parser.add_argument('--disable_gpu', required=False, default=False, action=\"store_true\", help=\"(Optional) Disables use of the GPU for preprocessing.\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"Preprocess a dataset for training a particle segmentation model.\")\n",
    "\n",
    "#     names = args.name\n",
    "\n",
    "#     if names is None:\n",
    "#         names = utils.load_filepaths(join(args.input, \"images\"), extension=\".nii.gz\", return_path=False, return_extension=False)\n",
    "\n",
    "#     print(\"Samples: \", names)\n",
    "#     print(\"Num samples: \", len(names))\n",
    "\n",
    "#     dataset_name = \"Task{}_ParticleSeg3D\".format(str(args.task).zfill(3))\n",
    "\n",
    "#     preprocess_all(args.input, names, args.output, args.target_spacing, args.target_particle_size, dataset_name, args.processes, args.thickness, not args.disable_gpu, args.zscore)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def setup_paths(dir_location, is_original_data, task):\n",
    "    if dir_location.lower() == 'internal':\n",
    "        base_path = r'C:\\Senior_Design'\n",
    "    elif dir_location.lower() == 'external':\n",
    "        base_path = r'D:\\Senior_Design'\n",
    "    elif dir_location.lower() == 'cloud':\n",
    "        base_path = r'C:\\Users\\dchen\\OneDrive - University of Connecticut\\Courses\\Year 4\\Fall 2024\\BME 4900 and 4910W (Kumavor)\\Python\\Files'\n",
    "    elif dir_location.lower() == 'refine':\n",
    "        base_path = r'D:\\Darren\\Files'\n",
    "    else:\n",
    "        raise ValueError('Invalid directory location type')\n",
    "    \n",
    "    base_input_path = os.path.join(base_path, 'database')\n",
    "    if is_original_data:\n",
    "        input_path = os.path.join(base_input_path, 'orignal_dataset', 'training', 'Task' + str(task))\n",
    "    else:\n",
    "        input_path = os.path.join(base_input_path, 'tablet_dataset', 'training', 'Task' + str(task))\n",
    "    \n",
    "    if not os.path.isdir(input_path):\n",
    "        raise ValueError(\"This input path is not valid:\\n\" + input_path)\n",
    "\n",
    "    output_path = os.path.join(base_path, 'training', 'nnUNet_preprocessed')\n",
    "    if not os.path.isdir(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    print('Paths set')\n",
    "    return input_path, output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set\n",
      "Samples:  ['4_GenericD12_Aug1']\n",
      "Num samples:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 956, 954)\n",
      "[975, 1639, 1635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:21<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Shapes of P1 and P2\n",
      "[486, 1639, 1635]\n",
      "[489, 1639, 1635]\n",
      "Shapes of P1 and P2\n",
      "torch.Size([1, 1, 486, 1639, 1635])\n",
      "torch.Size([1, 1, 489, 1639, 1635])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.73 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\Darren\\Senior_Design\\particleseg3d_source\\train\\utils.py:85\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(image, target_shape, seg, gpu, smooth_seg, processes, desc, disable, device)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ZEISS\\anaconda3\\envs\\Senior_Design_py310_source\\lib\\site-packages\\torch\\nn\\functional.py:4068\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4067\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_trilinear3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum samples: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(names))\n\u001b[0;32m     27\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ParticleSeg3D\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(task)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 29\u001b[0m \u001b[43mpreprocess_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_spacing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_particle_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthickness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbordercore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_zarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 65\u001b[0m, in \u001b[0;36mpreprocess_all\u001b[1;34m(load_dir, names, save_dir, target_spacing, target_particle_size_in_pixel, dataset_name, processes, border_thickness_in_pixel, gpu, bordercore, save_zarr, device)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(names))):\n\u001b[1;32m---> 65\u001b[0m         \u001b[43mpreprocess_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_load_filepaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_load_filepaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_load_filepaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_load_filepaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmetadata_load_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_load_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_save_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msemantic_seg_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_seg_save_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_seg_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_seg_save_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msemantic_seg_zarr_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_seg_zarr_save_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_seg_zarr_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_seg_zarr_save_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtarget_spacing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_spacing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_particle_size_in_pixel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_particle_size_in_pixel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mborder_thickness_in_pixel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mborder_thickness_in_pixel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbordercore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbordercore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_zarr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_zarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     ptqdm(preprocess_single, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(names)), processes, names\u001b[38;5;241m=\u001b[39mnames, image_load_filepaths\u001b[38;5;241m=\u001b[39mimage_load_filepaths,\n\u001b[0;32m     73\u001b[0m               seg_load_filepaths\u001b[38;5;241m=\u001b[39mseg_load_filepaths, metadata_load_filepath\u001b[38;5;241m=\u001b[39mmetadata_load_filepath,\n\u001b[0;32m     74\u001b[0m               image_save_dir\u001b[38;5;241m=\u001b[39mimage_save_dir, semantic_seg_save_dir\u001b[38;5;241m=\u001b[39msemantic_seg_save_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m               target_particle_size_in_pixel\u001b[38;5;241m=\u001b[39mtarget_particle_size_in_pixel,\n\u001b[0;32m     78\u001b[0m               border_thickness_in_pixel\u001b[38;5;241m=\u001b[39mborder_thickness_in_pixel, gpu\u001b[38;5;241m=\u001b[39mgpu, bordercore\u001b[38;5;241m=\u001b[39mbordercore, save_zarr\u001b[38;5;241m=\u001b[39msave_zarr, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[1], line 152\u001b[0m, in \u001b[0;36mpreprocess_single\u001b[1;34m(i, names, image_load_filepaths, seg_load_filepaths, metadata_load_filepath, image_save_dir, semantic_seg_save_dir, instance_seg_save_dir, semantic_seg_zarr_save_dir, instance_seg_zarr_save_dir, target_spacing, target_particle_size_in_pixel, border_thickness_in_pixel, gpu, bordercore, save_zarr, device)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(target_patch_size_in_pixel)\n\u001b[1;32m--> 152\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_patch_size_in_pixel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    156\u001b[0m patch_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)\n",
      "File \u001b[1;32md:\\Darren\\Senior_Design\\particleseg3d_source\\train\\utils.py:87\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(image, target_shape, seg, gpu, smooth_seg, processes, desc, disable, device)\u001b[0m\n\u001b[0;32m     85\u001b[0m         image \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39minterpolate(image, target_shape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 87\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate_on_multiple_gpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m smooth_seg:\n",
      "File \u001b[1;32md:\\Darren\\Senior_Design\\particleseg3d_source\\train\\utils.py:49\u001b[0m, in \u001b[0;36minterpolate_on_multiple_gpus\u001b[1;34m(image, target_shape, mode, device_0, device_1)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_part1_resampled\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_part2_resampled\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 49\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_part1_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_part2_resampled\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_image\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.73 GiB. GPU "
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from os.path import join\n",
    "\n",
    "# Set argument values directly\n",
    "input_path = \"/absolute/path/to/dataset\"  # Absolute path to the directory containing 'metadata.json', 'images', and either 'instance_seg' or 'border_core' directories\n",
    "output_path = \"/absolute/path/to/output\"  # Absolute path where the preprocessed dataset will be stored\n",
    "names = ['4_GenericD12_Aug1'] # None  # List of image names (without extensions) to use for training; None to use all available images\n",
    "task = '501'  # Task ID (int) assigned to the dataset\n",
    "target_spacing = 0.1  # Target spacing in millimeters\n",
    "target_particle_size = 60  # Target particle size in pixels\n",
    "processes = None  # Number of processes for parallel processing; None disables multiprocessing\n",
    "thickness = 2  # Border thickness in pixels (erosion only)\n",
    "disable_gpu = False  # If True, disables GPU usage for preprocessing\n",
    "bordercore = True # Used if the border-core representation is used instead of instance segmentation.\n",
    "save_zarr = False # Used to determine if Zarr files should be saved.\n",
    "device = 1 # Value indicating which GPU to use.\n",
    "\n",
    "# Paths\n",
    "input_path, output_path = setup_paths('refine', False, task)\n",
    "\n",
    "if names is None:\n",
    "    names = utils.load_filepaths(join(input_path, \"images\"), extension=\".nii.gz\", return_path=False, return_extension=False)\n",
    "\n",
    "print(\"Samples: \", names)\n",
    "print(\"Num samples: \", len(names))\n",
    "\n",
    "dataset_name = \"Task{}_ParticleSeg3D\".format(str(task).zfill(3))\n",
    "\n",
    "preprocess_all(input_path, names, output_path, target_spacing, target_particle_size, dataset_name, processes, thickness, not disable_gpu, bordercore, save_zarr, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Senior_Design_py310_source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
