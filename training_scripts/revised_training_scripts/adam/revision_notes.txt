==========================================================================================================================

\\wsl.localhost\Ubuntu\home\dchen\anaconda3\envs\Senior_Design_py310_source\lib\python3.10\site-packages\nnunet\training\network_training\nnUNetTrainerV2_ParticleSeg3D.py

- Change (Line 16): self.raw_data_dir = join("/", *nnunet_datasets_dir, "nnUNet_raw_data", "nnUNet_raw_data", task_name)
- To (Line 16): self.raw_data_dir = join("/", *nnunet_datasets_dir, "nnUNet_raw_data_base", "nnUNet_raw_data", task_name)
- Why: The path name is wrong lol.

Transferred revised version of above script on 3/23/25.

==========================================================================================================================

\\wsl.localhost\Ubuntu\home\dchen\anaconda3\envs\Senior_Design_py310_source\lib\python3.10\site-packages\nnunet\training\network_training\network_trainer.py

- Change (Line 122): self.save_every = 50
- To (Line 122): self.save_every = 1
- Why: Make sure to save after very epoch.

Transferred revised version of above script on 3/23/25.

==========================================================================================================================

\\wsl.localhost\Ubuntu\home\dchen\anaconda3\envs\Senior_Design_py310_source\lib\python3.10\site-packages\nnunet\training\network_training\uuNetTrainerV2.py

- Add (Line 37): from torch.optim import lr_scheduler

- Change (Line 50): self.initial_lr = 1e-2
- To (Lines 50-56):
        self.optim_name = 'adam'
        if self.optim_name == 'sgd':
            self.initial_lr = 1e-2 # Original LR for SGD
        elif self.optim_name == 'adam':
            self.initial_lr = 3e-4 # Original LR for SGD
        else:
            raise ValueError('Not a valid optimizer!')
- Why: To enable setting SGD or Adam optimizer and their respective LR.

- Change (Lines 171-175):
    def initialize_optimizer_and_scheduler(self):
        assert self.network is not None, "self.initialize_network must be called first"
        self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay,
                                         momentum=0.99, nesterov=True)
        self.lr_scheduler = None
- To (Lines 171-186): 
    def initialize_optimizer_and_scheduler(self):
        assert self.network is not None, "self.initialize_network must be called first"
        
        if self.optim_name == 'sgd':
            self.optimizer = torch.optim.SGD(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay,
                                            momentum=0.99, nesterov=True)
            self.lr_scheduler = None
        elif self.optim_name == 'adam':
            self.optimizer = torch.optim.Adam(self.network.parameters(), self.initial_lr, weight_decay=self.weight_decay,
                                          amsgrad=True)
            self.lr_scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2,
                                                           patience=self.lr_scheduler_patience,
                                                           verbose=True, threshold=self.lr_scheduler_eps,
                                                           threshold_mode="abs")
        else:
            raise ValueError('Not a valid optimizer!')
- Why: To enable initialization between SGD and Adam optimizer (based on code from nnUNet_Trainer.py).

- Change (Lines 409-424):
    def maybe_update_lr(self, epoch=None):
        """
        if epoch is not None we overwrite epoch. Else we use epoch = self.epoch + 1

        (maybe_update_lr is called in on_epoch_end which is called before epoch is incremented.
        herefore we need to do +1 here)

        :param epoch:
        :return:
        """
        if epoch is None:
            ep = self.epoch + 1
        else:
            ep = epoch
        self.optimizer.param_groups[0]['lr'] = poly_lr(ep, self.max_num_epochs, self.initial_lr, 0.9)
        self.print_to_log_file("lr:", np.round(self.optimizer.param_groups[0]['lr'], decimals=6))
- To (Lines 409-434):
    def maybe_update_lr(self, epoch=None):
        """
        if epoch is not None we overwrite epoch. Else we use epoch = self.epoch + 1

        (maybe_update_lr is called in on_epoch_end which is called before epoch is incremented.
        herefore we need to do +1 here)

        :param epoch:
        :return:
        """
        if epoch is None:
            ep = self.epoch + 1
        else:
            ep = epoch
        
        if self.lr_scheduler is None: # For SGD with polynomial decay
            self.optimizer.param_groups[0]['lr'] = poly_lr(ep, self.max_num_epochs, self.initial_lr, 0.9)
            self.print_to_log_file("lr:", np.round(self.optimizer.param_groups[0]['lr'], decimals=6))
        else:
            assert isinstance(self.lr_scheduler, (lr_scheduler.ReduceLROnPlateau, lr_scheduler._LRScheduler))

            if isinstance(self.lr_scheduler, lr_scheduler.ReduceLROnPlateau):
                # lr scheduler is updated with moving average val loss. should be more robust
                self.lr_scheduler.step(self.train_loss_MA)
            else:
                self.lr_scheduler.step(self.epoch + 1)
- Why: To update LR correctly (based on code from nnUNet_Trainer.py).

Transferred revised version of above script on 3/23/25.

==========================================================================================================================